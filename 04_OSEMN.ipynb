{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science is OSEMN\n",
    "\n",
    "According to a popular model, the elements of data science are\n",
    "\n",
    "* Obtaining data\n",
    "* Scrubbing data\n",
    "* Exploring data\n",
    "* Modeling data\n",
    "* iNterpreting data\n",
    "\n",
    "and hence the acronym OSEMN, pronounced as “Awesome”.\n",
    "\n",
    "We will start with the **O**, moving towards the rest later, but first let's have a quick look at what it all boils down to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.loadtxt('populations.txt')\n",
    "year, hares, lynxes, carrots = data.T # trick: columns to variables\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.axes([0.2, 0.1, 0.5, 0.8]) \n",
    "plt.plot(year, hares, year, lynxes, year, carrots) \n",
    "plt.legend(('Hare', 'Lynx', 'Carrot'), loc=(1.05, 0.5)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting the data a clear (and reasonable) correlations between pray and predator becomes evident. How can it be quantified? Is that statistical significant? What about the correlation between carrots and hares? Is that evident? Is that significant?\n",
    "\n",
    "Finding correlations in data is the main goal of data science, though that is not the end of the story: as this precious [site](http://tylervigen.com/spurious-correlations) demonstrates, **correlations is not causation**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Exercise*: write an algorithm that determins and quantifies a correlation between two time series. Use as an example the hare-lynx-carrot dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining and processing (remote) data\n",
    "\n",
    "Accessing data is a really serious business. Data can sit on public or on remote machines. In the case of the former, things may be straightforward, whereas in the latter case you need to worry about a few things.\n",
    "\n",
    "In both cases, depending on the size of the dataset, the managment of the dataset can become extremely complicated. We won't deal here with large datasets, which require a whole course per se.., but still care should be put. In particular, it is not wise to keep (and even worse commit) data into a git repository!\n",
    "\n",
    "The suggestion is then to create a directory somewhere and copy the example datasets there. From a terminal:\n",
    "\n",
    "```bash\n",
    "\n",
    "# create a data directory in your home directory\n",
    "mkdir ~/data/\n",
    "\n",
    "# check the content (it's empty now of course)\n",
    "ls -ltr ~/data/\n",
    "\n",
    "# in the case you need to move there:\n",
    "cd ~/data/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data from a server\n",
    "\n",
    "A nice set of interesting datasets can be found on this [server](https://archive.ics.uci.edu/ml/datasets.html?sort=nameUp&view=list) that collects training/test data for machine learning developments. Several of those pertein physical sciences, it is worth browsing through those.\n",
    "\n",
    "You can download any of those, in the following we will consider a dataset from the MAGIC experiment. For that we will the `wget` command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-12 01:03:00--  https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data\n",
      "Risoluzione di archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
      "Connessione a archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 1477391 (1.4M) [text/plain]\n",
      "Salvataggio in: \"/Users/mzanetti/data/magic04.data.1\"\n",
      "\n",
      "magic04.data.1      100%[===================>]   1.41M   782KB/s    in 1.8s    \n",
      "\n",
      "2018-11-12 01:03:03 (782 KB/s) - \"/Users/mzanetti/data/magic04.data.1\" salvato [1477391/1477391]\n",
      "\n",
      "--2018-11-12 01:03:03--  https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names\n",
      "Risoluzione di archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.249\n",
      "Connessione a archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.249|:443... connesso.\n",
      "Richiesta HTTP inviata, in attesa di risposta... 200 OK\n",
      "Lunghezza: 5400 (5.3K) [text/plain]\n",
      "Salvataggio in: \"/Users/mzanetti/data/magic04.names.1\"\n",
      "\n",
      "magic04.names.1     100%[===================>]   5.27K  --.-KB/s    in 0s      \n",
      "\n",
      "2018-11-12 01:03:04 (80.5 MB/s) - \"/Users/mzanetti/data/magic04.names.1\" salvato [5400/5400]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the dataset and its description on the proper data directory\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P ~/data/\n",
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P ~/data/    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to download and load remote files via their url's directly from within python (and thus on a jupyter session). This is a rather powerful tool as it allows http communications, IO streaming and so on.\n",
    "\n",
    "Care should be put as the dataset is stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "url ='https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names'\n",
    "with urllib.request.urlopen(url) as data_file:\n",
    "    #print (data_file.read(300))\n",
    "    for line in data_file:\n",
    "        print (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Copy data from a remote machine\n",
    "\n",
    "Often datasets are not available on websites but rather they are sitting on some remote machine. Several tools are there that can allow you to get hold off remote data, even from within python (e.g. [paramiko](https://www.paramiko.org/)), but best in this case is to get a local copy. E.g. from a terminal:\n",
    "\n",
    "```bash\n",
    "scp lemma@lxplus.cern.ch:/eos/project/l/lemma/data/2018/raw/Run000333/data_000637.* ~/data/\n",
    "```\n",
    "\n",
    "by issuing that command you are immediately exposed to the most relevant problem in obtaining the data: permissions/authorization.\n",
    "\n",
    "Secondily (essentially a further consequence of the same issue), the remote machine itself may have accessibility restrictions, e.g. being behind a firewall. In that case you may need to use a tunnel:\n",
    "\n",
    "``` bash\n",
    "ssh -L 1234:<address of R known to G>:22 <user at G>@<address of G> \n",
    "\n",
    "scp -P 1234 <user at R>@127.0.0.1:/path/to/file file-name-to-be-copied\n",
    "```\n",
    "\n",
    "In summary, just getting the data is a complicated business."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formats\n",
    "\n",
    "datasets can be stored in a gazillion different ways, often they have formats which are application dependent, even though more and more standards are being established. Python have \"readers\" for most of the formats, another reason for being the optimal programming language for data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text files \n",
    "\n",
    "Plain text files are commonly used for \"readibility\", at the price of a very poor storing efficiency due to their low entropy. [UTF-8](https://en.wikipedia.org/wiki/UTF-8) is the most common encoding.\n",
    "\n",
    "Reading (and writing) text files in python is straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '~/data/magic04.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-24c62e7f603b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"~/data/magic04.data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;31m# print-out the whole file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# print (f.read())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '~/data/magic04.data'"
     ]
    }
   ],
   "source": [
    "file_name = \"~/data/magic04.data\"\n",
    "with open(file_name) as f:\n",
    "    # print-out the whole file\n",
    "    # print (f.read()) \n",
    "    for line in f:\n",
    "        # print line by line\n",
    "        print (line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are lucky text files are already framed into a defined structured, in a \"table-like\" manner. These files are colled \"comma separated values\" (csv), even though the separator  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('magic04.data') as data_file:\n",
    "    for line in csv.reader(data_file):\n",
    "        fLength,fWidth,fSize,\\\n",
    "        fConc,fConc1,fAsym,\\\n",
    "        fM3Long,fM3Trans,fAlpha,fDist = map(float,line[:-1])\n",
    "        category = line[-1]\n",
    "        print (fLength,fWidth,fSize,fConc,fConc1,fAsym,fM3Long,fM3Trans,fAlpha,fDist)\n",
    "        print (category)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file example.json\n",
    "{\n",
    "    \"glossary\": {\n",
    "        \"title\": \"example glossary\",\n",
    "            \"GlossDiv\": {\n",
    "            \"title\": \"S\",\n",
    "                    \"GlossList\": {\n",
    "                \"GlossEntry\": {\n",
    "                    \"ID\": \"SGML\",\n",
    "                                    \"SortAs\": \"SGML\",\n",
    "                                    \"GlossTerm\": \"Standard Generalized Markup Language\",\n",
    "                                    \"Acronym\": \"SGML\",\n",
    "                                    \"Abbrev\": \"ISO 8879:1986\",\n",
    "                                    \"GlossDef\": {\n",
    "                        \"para\": \"A meta-markup language, used to create markup languages such as DocBook.\",\n",
    "                                            \"GlossSeeAlso\": [\"GML\", \"XML\"]\n",
    "                    },\n",
    "                                    \"GlossSee\": \"markup\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat example.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Title of Database: MAGIC gamma telescope data 2004\r\n",
      "\r\n",
      "2. Sources:\r\n",
      "\r\n",
      "   (a) Original owner of the database:\r\n",
      "\r\n",
      "       R. K. Bock\r\n",
      "       Major Atmospheric Gamma Imaging Cherenkov Telescope project (MAGIC)\r\n",
      "       http://wwwmagic.mppmu.mpg.de\r\n",
      "       rkb@mail.cern.ch\r\n",
      "\r\n",
      "   (b) Donor:\r\n",
      "\r\n",
      "       P. Savicky\r\n",
      "       Institute of Computer Science, AS of CR\r\n",
      "       Czech Republic\r\n",
      "       savicky@cs.cas.cz\r\n",
      "\r\n",
      "   (c) Date received: May 2007\r\n",
      "\r\n",
      "3. Past Usage:\r\n",
      "\r\n",
      "   (a) Bock, R.K., Chilingarian, A., Gaug, M., Hakl, F., Hengstebeck, T.,\r\n",
      "       Jirina, M., Klaschka, J., Kotrc, E., Savicky, P., Towers, S.,\r\n",
      "       Vaicilius, A., Wittek W. (2004).\r\n",
      "       Methods for multidimensional event classification: a case study\r\n",
      "       using images from a Cherenkov gamma-ray telescope.\r\n",
      "       Nucl.Instr.Meth. A, 516, pp. 511-528.\r\n",
      "\r\n",
      "   (b) P. Savicky, E. Kotrc.\r\n",
      "       Experimental Study of Leaf Confidences for Random Forest.\r\n",
      "       Proceedings of COMPSTAT 2004, In: Computational Statistics.\r\n",
      "       (Ed.: Antoch J.) - Heidelberg, Physica Verlag 2004, pp. 1767-1774.\r\n",
      "\r\n",
      "   (c) J. Dvorak, P. Savicky.\r\n",
      "       Softening Splits in Decision Trees Using Simulated Annealing.\r\n",
      "       Proceedings of ICANNGA 2007, Warsaw, (Ed.: Beliczynski et. al),\r\n",
      "       Part I, LNCS 4431, pp. 721-729.\r\n",
      "\r\n",
      "4. Relevant Information:\r\n",
      "\r\n",
      "   The data are MC generated (see below) to simulate registration of high energy\r\n",
      "   gamma particles in a ground-based atmospheric Cherenkov gamma telescope using the\r\n",
      "   imaging technique. Cherenkov gamma telescope observes high energy gamma rays,\r\n",
      "   taking advantage of the radiation emitted by charged particles produced\r\n",
      "   inside the electromagnetic showers initiated by the gammas, and developing in the\r\n",
      "   atmosphere. This Cherenkov radiation (of visible to UV wavelengths) leaks\r\n",
      "   through the atmosphere and gets recorded in the detector, allowing reconstruction\r\n",
      "   of the shower parameters. The available information consists of pulses left by\r\n",
      "   the incoming Cherenkov photons on the photomultiplier tubes, arranged in a\r\n",
      "   plane, the camera. Depending on the energy of the primary gamma, a total of\r\n",
      "   few hundreds to some 10000 Cherenkov photons get collected, in patterns\r\n",
      "   (called the shower image), allowing to discriminate statistically those\r\n",
      "   caused by primary gammas (signal) from the images of hadronic showers\r\n",
      "   initiated by cosmic rays in the upper atmosphere (background).\r\n",
      "\r\n",
      "   Typically, the image of a shower after some pre-processing is an elongated\r\n",
      "   cluster. Its long axis is oriented towards the camera center if the shower axis\r\n",
      "   is parallel to the telescope's optical axis, i.e. if the telescope axis is\r\n",
      "   directed towards a point source. A principal component analysis is performed\r\n",
      "   in the camera plane, which results in a correlation axis and defines an ellipse.\r\n",
      "   If the depositions were distributed as a bivariate Gaussian, this would be\r\n",
      "   an equidensity ellipse. The characteristic parameters of this ellipse\r\n",
      "   (often called Hillas parameters) are among the image parameters that can be\r\n",
      "   used for discrimination. The energy depositions are typically asymmetric\r\n",
      "   along the major axis, and this asymmetry can also be used in discrimination.\r\n",
      "   There are, in addition, further discriminating characteristics, like the\r\n",
      "   extent of the cluster in the image plane, or the total sum of depositions.\r\n",
      "\r\n",
      "   The data set was generated by a Monte Carlo program, Corsika, described in \r\n",
      "      D. Heck et al., CORSIKA, A Monte Carlo code to simulate extensive air showers,\r\n",
      "      Forschungszentrum Karlsruhe FZKA 6019 (1998).\r\n",
      "   The program was run with parameters allowing to observe events with energies down\r\n",
      "   to below 50 GeV.\r\n",
      "\r\n",
      "5. Number of Instances: 19020\r\n",
      "\r\n",
      "6. Number of Attributes: 11 (including the class)\r\n",
      "\r\n",
      "7. Attribute information:\r\n",
      "\r\n",
      "    1.  fLength:  continuous  # major axis of ellipse [mm]\r\n",
      "    2.  fWidth:   continuous  # minor axis of ellipse [mm] \r\n",
      "    3.  fSize:    continuous  # 10-log of sum of content of all pixels [in #phot]\r\n",
      "    4.  fConc:    continuous  # ratio of sum of two highest pixels over fSize  [ratio]\r\n",
      "    5.  fConc1:   continuous  # ratio of highest pixel over fSize  [ratio]\r\n",
      "    6.  fAsym:    continuous  # distance from highest pixel to center, projected onto major axis [mm]\r\n",
      "    7.  fM3Long:  continuous  # 3rd root of third moment along major axis  [mm] \r\n",
      "    8.  fM3Trans: continuous  # 3rd root of third moment along minor axis  [mm]\r\n",
      "    9.  fAlpha:   continuous  # angle of major axis with vector to origin [deg]\r\n",
      "   10.  fDist:    continuous  # distance from origin to center of ellipse [mm]\r\n",
      "   11.  class:    g,h         # gamma (signal), hadron (background)\r\n",
      "\r\n",
      "8. Missing Attribute Values: None\r\n",
      "\r\n",
      "9. Class Distribution:\r\n",
      "\r\n",
      "   g = gamma (signal):     12332\r\n",
      "   h = hadron (background): 6688\r\n",
      "\r\n",
      "   For technical reasons, the number of h events is underestimated.\r\n",
      "   In the real data, the h class represents the majority of the events.\r\n",
      "\r\n",
      "   The simple classification accuracy is not meaningful for this data, since\r\n",
      "   classifying a background event as signal is worse than classifying a signal\r\n",
      "   event as background. For comparison of different classifiers an ROC curve\r\n",
      "   has to be used. The relevant points on this curve are those, where the\r\n",
      "   probability of accepting a background event as signal is below one of the\r\n",
      "   following thresholds: 0.01, 0.02, 0.05, 0.1, 0.2 depending on the required\r\n",
      "   quality of the sample of the accepted events for different experiments.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# print the description. This can (and better) be done from a terminal\n",
    "!cat ~/data/magic04.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
